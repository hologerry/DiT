# %% [markdown]
# # Scalable Diffusion Models with Transformer (DiT)
#
# This notebook samples from pre-trained DiT models. DiTs are class-conditional latent diffusion models trained on ImageNet that use transformers in place of U-Nets as the DDPM backbone. DiT outperforms all prior diffusion models on the ImageNet benchmarks.
#
# [Project Page](https://www.wpeebles.com/DiT) | [HuggingFace Space](https://huggingface.co/spaces/wpeebles/DiT) | [Paper](http://arxiv.org/abs/2212.09748) | [GitHub](github.com/facebookresearch/DiT)

# %% [markdown]
# # 1. Setup
#
# We recommend using GPUs (Runtime > Change runtime type > Hardware accelerator > GPU). Run this cell to clone the DiT GitHub repo and setup PyTorch. You only have to run this once.

# %%
# !git clone https://github.com/facebookresearch/DiT.git
# import DiT, os
# os.chdir('DiT')
# os.environ['PYTHONPATH'] = '/env/python:/content/DiT'
# !pip install diffusers timm --upgrade
# DiT imports:
import torch

from diffusers.models import AutoencoderKL
from IPython.display import display
from PIL import Image
from torchvision.utils import save_image

from diffusion import create_diffusion
from download import find_model
from models import DiT_XL_2


torch.set_grad_enabled(False)
device = "cuda" if torch.cuda.is_available() else "cpu"
if device == "cpu":
    print("GPU not found. Using CPU instead.")

# %% [markdown]
# # Download DiT-XL/2 Models
#
# You can choose between a 512x512 model and a 256x256 model. You can swap-out the LDM VAE, too.

# %%
image_size = 256  # @param [256, 512]
vae_model = "stabilityai/sd-vae-ft-ema"  # @param ["stabilityai/sd-vae-ft-mse", "stabilityai/sd-vae-ft-ema"]
latent_size = int(image_size) // 8
# Load model:
model = DiT_XL_2(input_size=latent_size).to(device)
state_dict = find_model(f"DiT-XL-2-{image_size}x{image_size}.pt")
model.load_state_dict(state_dict)
model.eval()  # important!
vae = AutoencoderKL.from_pretrained(vae_model).to(device)

# %% [markdown]
# # 2. Sample from Pre-trained DiT Models
#
# You can customize several sampling options. For the full list of ImageNet classes, [check out this](https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a).

# %%
# Set user inputs:
seed = 0  # @param {type:"number"}
torch.manual_seed(seed)
num_sampling_steps = 250  # @param {type:"slider", min:0, max:1000, step:1}
cfg_scale = 4  # @param {type:"slider", min:1, max:10, step:0.1}
class_labels = 207, 360, 387, 974, 88, 979, 417, 279  # @param {type:"raw"}
samples_per_row = 4  # @param {type:"number"}

# Create diffusion object:
diffusion = create_diffusion(str(num_sampling_steps))

# Create sampling noise:
n = len(class_labels)
z = torch.randn(n, 4, latent_size, latent_size, device=device)
y = torch.tensor(class_labels, device=device)

# Setup classifier-free guidance:
z = torch.cat([z, z], 0)
y_null = torch.tensor([1000] * n, device=device)
y = torch.cat([y, y_null], 0)
model_kwargs = dict(y=y, cfg_scale=cfg_scale)

# Sample images:
samples = diffusion.p_sample_loop(
    model.forward_with_cfg, z.shape, z, clip_denoised=False, model_kwargs=model_kwargs, progress=True, device=device
)
samples, _ = samples.chunk(2, dim=0)  # Remove null class samples
samples = vae.decode(samples / 0.18215).sample

# Save and display images:
save_image(samples, "sample.png", nrow=int(samples_per_row), normalize=True, value_range=(-1, 1))
samples = Image.open("sample.png")
display(samples)
